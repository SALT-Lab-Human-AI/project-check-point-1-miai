# AI Mock Interview Research Summaries

This repository contains structured notes and insights from recent research papers exploring the use of AI-driven mock interviews for candidate evaluation, employability, and confidence building.

---

## üìÑ Paper 1: *AI-driven mock interview assessment: leveraging generative language models for automated evaluation*  
**Citation:** Uppalapati, P. J., Dabbiru, M., & Kasukurthi, V. R. (2025). *AI-driven mock interview assessment: leveraging generative language models for automated evaluation.* International Journal of Machine Learning and Cybernetics, 1-23.  
**Link:** [https://doi.org/10.1007/s13042-025-02529-9](https://doi.org/10.1007/s13042-025-02529-9)

### Summary (4‚Äì6 sentences)  
This paper explores the use of generative language models (GLMs) to automate candidate evaluation in mock interviews. The authors propose a framework that uses large language models to assess interview responses along multiple dimensions, such as content relevance, communication clarity, and behavioral indicators. Experiments demonstrated that GLM-based assessments strongly correlated with human evaluator scores, while also offering scalability and cost-efficiency. The system additionally provided qualitative feedback, helping candidates identify areas for improvement. The study concludes that GLMs can serve as a reliable complement or alternative to human assessors in interview preparation.

### 3 Insights I Learned  
1. GLMs can approximate human-like evaluation of interview responses with high consistency.  
2. Automated feedback can save time and resources, making interview coaching scalable.  
3. Combining quantitative scoring with qualitative narrative feedback increases candidate trust and usefulness.  

### 2 Limitations/Risks  
1. AI models may still misinterpret nuanced or culturally specific responses, leading to unfair scoring.  
2. Over-dependence on AI assessments could reduce the value of human mentorship and personalized guidance.  

### 1 Concrete Idea for Our Project  
Mocksy could integrate a dual-layer evaluation system: automated scoring powered by a generative model, paired with explanatory feedback narratives, to make results more actionable and human-like for candidates.  

---

## üìÑ Paper 2: *Can Interviewees Fake Out AI? Comparing the Susceptibility and Mechanisms of Faking Across Self‚ÄêReports, Human Interview Ratings, and AI Interview Ratings*  
**Citation:** Hickman, L., Liff, J., Willis, C., & Kim, E. (2025). *Can Interviewees Fake Out AI? Comparing the Susceptibility and Mechanisms of Faking Across Self‚ÄêReports, Human Interview Ratings, and AI Interview Ratings.* International Journal of Selection and Assessment, 33(2), e70014.  
**Link:** [https://doi.org/10.1111/ijsa.70014](https://doi.org/10.1111/ijsa.70014)

### Summary (4‚Äì6 sentences)  
This study examines whether job candidates can ‚Äúfake‚Äù responses to perform better in AI-based interviews compared to self-reports and human-rated interviews. The researchers compared candidate behaviors across three assessment modalities: self-report questionnaires, structured human interviews, and AI-driven interviews. Findings indicate that while self-reports are highly vulnerable to faking, AI interview ratings and human ratings were less susceptible, though not immune. The AI system detected some inconsistencies better than humans, but also missed nuanced cues. Overall, the study shows that AI interviews offer promise in resisting manipulation, but require ongoing refinement to balance fairness and reliability.

### 3 Insights I Learned  
1. AI interviews may be more resistant to faking than traditional self-reports, making them useful for high-stakes assessments.  
2. Candidates still engage in impression management, but AI systems can detect certain inconsistencies better than humans.  
3. No method is perfect‚Äîmulti-method assessment (AI + human oversight) might be most reliable.  

### 2 Limitations/Risks  
1. AI systems may still be tricked by rehearsed or surface-level answers, leading to inflated ratings.  
2. Over-reliance on AI could create ethical risks if candidates are unfairly flagged or misjudged due to system bias.  

### 1 Concrete Idea for Our Project  
We could integrate faking detection features into Mocksy‚Äîfor example, analyzing speech hesitation patterns, response repetitiveness, or over-polished answers. This would make the platform more robust by flagging potentially inauthentic responses, helping users prepare realistically for interviews.  

---

## üìÑ Paper 3: *Empirical research on the application of AI mock interviews in enhancing graduate perceived employability: a case study in Hangzhou, China*  
**Citation:** Shi, W., & Wang, D. (2025). *Empirical research on the application of AI mock interviews in enhancing graduate perceived employability: a case study in Hangzhou, China.* Education and Information Technologies, 1-24.  
**Link:** [https://doi.org/10.1007/s10639-025-13525-5](https://doi.org/10.1007/s10639-025-13525-5)

### Summary (4‚Äì6 sentences)  
This study explores how AI-driven mock interviews impact graduates‚Äô perceived employability in China. Using a case study methodology with graduates in Hangzhou, the researchers integrated AI mock interview tools into career development programs. Data were collected through surveys and performance evaluations before and after the intervention. Results demonstrated that AI mock interviews significantly boosted graduates‚Äô confidence, self-presentation, and job readiness. The study concludes that AI-based practice can play a critical role in bridging the gap between academic training and employability.

### 3 Insights I Learned  
1. AI mock interviews not only improve technical performance but also enhance self-efficacy and employability perceptions.  
2. Embedding AI interview practice into career services programs can scale preparation for large student populations.  
3. Perceived employability is a psychological as well as skill-based construct, influenced by confidence-building interventions.  

### 2 Limitations/Risks  
1. The findings are based on a single case study in China, which may limit generalizability across cultures or industries.  
2. Reliance on self-reported employability measures may not fully reflect real-world hiring outcomes.  

### 1 Concrete Idea for Our Project  
Mocksy could include a ‚Äúconfidence and employability tracker‚Äù that measures how candidates feel about their readiness before and after multiple sessions. This would allow users to see psychological growth alongside skill development.  

---

## üìÑ Paper 4: *Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence*  
**Citation:** Gomez, N., Batham, S. S., Volonte, M., & Do, T. D. (2025). *Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence.* arXiv preprint arXiv:2506.16542.  
**Link:** [https://doi.org/10.48550/arXiv.2506.16542](https://doi.org/10.48550/arXiv.2506.16542)

### Summary (4‚Äì6 sentences)  
This paper investigates whether AI-driven mock technical interviews can improve student preparedness and confidence for real-world job interviews. The authors design and deploy a virtual interviewing platform that simulates technical interview scenarios, capturing both coding ability and communication skills. The study involved students using the system and comparing outcomes against traditional preparation methods. Results showed that students reported greater confidence, reduced anxiety, and improved readiness when practicing with the AI interviewer. The findings highlight AI‚Äôs potential as a scalable and accessible interview preparation tool for educational settings.

### 3 Insights I Learned  
1. Mock interviews powered by AI can reduce interview anxiety while providing structured, repeatable practice.  
2. Combining technical and behavioral evaluation gives a holistic view of student performance.  
3. AI-driven platforms can serve as scalable solutions in academic institutions where human interviewers are limited.  

### 2 Limitations/Risks  
1. The AI interviewer may lack nuance in feedback, compared to experienced human mentors.  
2. Over-reliance on AI systems could lead students to develop formulaic responses, reducing adaptability in real interviews.  

### 1 Concrete Idea for Our Project  
We can design Mocksy to not only ask role-specific technical questions but also provide confidence-building feedback, helping candidates improve both their skills and mindset. For example, tracking growth in anxiety reduction or confidence levels over multiple mock sessions could be added as a progress feature.  